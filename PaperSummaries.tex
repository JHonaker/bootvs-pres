% Created 2017-04-18 Tue 12:50
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{John Honaker}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={John Honaker},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.0.5)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{Variable Selection with Bootstrap in GLM Model for Massive Data}
\label{sec:org6c1ea59}
arXiv:1612.01801v2

\subsection{Summary}
\label{sec:orgf5d5bf6}

Variable Selection with Bag of Little Bootstraps for Generalized Linear Regression

One method for variable selection in a linear regression setting is the LASSO. Traditional linear regression involves finding a solution, \(\beta\), that minimizes \(\| Y - X\beta \|^2_2\). In the LASSO, we trade a small bit of bias, determined by the choice of a regularizatieon hyperparameter, \(\lambda\), to \emph{shrink} the parameters toward zero, and even setting some to zero. The problem is equivalent to minimizing $$ \|Y - X\beta \|^2_2 + \lambda \| \beta \|_1.$$


We can find a solution by applying the Cyclic Coordinate Descent algorithm to the above equation.

One of the drawbacks of the major drawbacks of the LASSO is the lack of a measure of uncertainty of the estimate, in the sense of how sensitive it is to the observed data versus the true data generating mechanism. We can asses this variablity by using a bootstrap variable selection procedure. Essentially, we use bootstrap resamples and a model averaging voting scheme similar to random forests. A regression coefficent recieves a vote for each resample that estimates the parameter to be non-zero. The proportion of votes to resamples above a threshold is then used to select variables.

In the context of a massive dataset, the computational cost of even a small number of boostrap resamples becomes very large. However, given that we have a large dataset, we can use only a portion of our data for each replicate. This allows us to drastically reduce the computational cost while still obtaining a good bootstrap estimate.

\subsection{BLBVS}
\label{sec:org216ad25}

The authors introduce the Bag of Little Bootstraps variable selection method to asses the quality of the estimators and reduce the computational cost of the bootstrap variable selection procedure in the context of a massive dataset.

The algorithm for the BLBVS procedure is as follows. Partition the set of observed data \((x_1, ..., x_n)\) into \(s\) randomly selected subsets (called bags) of size \(b = \{n^\gamma : \gamma \in \(0, 1\) \}\) chosen \emph{without replacement}. Each subset produces \(r\) bootstrap resamples chosen \emph{with replacement} of size \(n\), the original sample size. Notice that each resample can only have at most \(b\) distinct data points.

Let \(\hat{\beta_{ij}}\) be a vetor of estimators for the \(j\text{th}\) resample of the \(i\text{th}\) subset, and \(\hat{\xi}_i\) be the standard error of the parameter estimation in the \(i\text{th}\) subset. The overall standard error \(\hat{\xi}\) is simply the average over the \(\hat{\xi}_i\).

To perform variable selection, we consider indicator functions, \(\mathbf{I} \left( \hat{\beta}_{ijk} \neq 0 \right)}\), to represent whether or the parameter estimate \(k\) in the \(j\text{th}\) resample of the \(i\text{th}\) subset was set to zero. The selection procedure is based on model averaging using a vote criterion similar to Random Forests. Each bootstrap sample is given an equally weighted vote, for a total of \(s \times r\) votes. A predictor is selected if it recieves a proportion of votes, \(p_k\), greater than some pre-chosen cutoff \(c\). Where \(p_k\) is defined as $$p_k = \frac{\sum^s_{i=1} \sum^r_{j=1} \mathbf{I}(\hat{\beta}_k (X_{ij}) \neq 0)}{s \times r}.$$

\subsection{BLBVS with Group Lasso Penalty for GLM}
\label{sec:org797a4ed}

The variable selection method (BLBVS) works perfectly well for a traditional linear regression with continuous predictors. However, it does not handle generalized linear models with categorical predictors or different link functions properly.

For a categorical predictor with \(k\) categories, \(k-1\) dummy variables must be added to the model. Under the original Lasso framework, the dummy variables are penalized individually. This may cause a partial choice of some of levels of the predictor. Ideally, all the dummy variables from a certain category should be chosen or eliminated together. An extension to the Lasso, called the Group Lasso, can overcome these issues by grouping sets of related dummy variables together and penalizing them together. The estimator is defined as $$\hat{\beta}_\lambda = argmin(\| Y - X \beta \|^2_2 + \lambda \sum^G_{g=1} \|\beta I_g \|_2)$$ where \(I_g\) is the index set of the \(g\text{th}\) group of variables, \(g = 1,...,G\) .

To extend the method to a GLM framework, the cost function must be extended to incorporate the link function. The first term of the Lasso cost function then becomes \(\| Y - g^{-1}(X\beta) \|^2_2\) insetad of simply \(\| Y - X\beta \|^2_2\). Consider the following setup of logistic regression for more details. Let \(Y_i\) be a binary response variable, and \(\mathbf{X}\) be the matrix of p independently and idendically distributed predictor variables. The predictor variables are divided into \(G\) groups. Each continuous predictor is its own group with \(df_g = 1\). Each set of dummy variables pertaining to a single categorical variable (with \(k\) levels) is collected as one group with \(df_g = k - 1\). \(\mathbf{X}\) can now be partitioned groupwise by columns so that the linear model becomes $$g(\mathbf{Y}) = log \left( \frac{p_\beta(\mathbf{X})}{1 - p_\beta (\mathbf{X})} \right) = \beta_0 + \sum^G_{g=1} X^T_g \beta_g $$ where \(p_\beta(x) = P_\beta(Y = 1 | x)\) and \(\beta_g \in \mathcal{R}^{df_g}\). This problem is equivalent to minimizing the convex function: $$S_\lambda(\beta) = -l(\beta) + \lambda \sum^G_{g=1} s(df_g) \|\beta_g \|_1$$ where \(l(\cdot)\) is the log-likelihood and \(s(\cdot)\) is chosen to rescale the regularization term with respect to the degrees of freedom in the group. Typically, the value of \(\lambda\) is chosen via cross-validation.

\subsection{{\bfseries\sffamily TODO} Simulations}
\label{sec:org4011891}

For the simulation analysis of the BLBVS method, they considered logistic regression with continuous independent predictors as the true model and Group Lasso as the penalty funciton. In this paper, they performed simulations considering \(b = n^\gamma\) with \(\gamma \in \left\{ 0.6, 0.7, 0.8 \right\}\) and let \(r = 100\) in each subset.

The simulated dataset was set to have \(n = 20,000\) and 35 continuous predictors goruped into 8 groups:

\begin{center}
\begin{tabular}{ll}
Index Var & Index Variables\\
\hline
\(\mathcat{I}_1\) & \(\left\{ 1, ..., 5 \right\}\)\\
\(\mathcat{I}_2\) & \(\left\{ 6, ..., 9 \right\}\)\\
\(\mathcat{I}_3\) & \(\left\{ 10, ..., 15 \right\}\)\\
\(\mathcat{I}_4\) & \(\left\{ 16, ..., 20 \right\}\)\\
\(\mathcat{I}_5\) & \(\left\{ 21, ..., 25 \right\}\)\\
\(\mathcat{I}_6\) & \(\left\{ 26, ..., 28 \right\}\)\\
\(\mathcat{I}_7\) & \(\left\{ 29, ..., 31 \right\}\)\\
\(\mathcat{I}_8\) & \(\left\{ 32, ..., 35 \right\}\)\\
\end{tabular}
\end{center}

Coefficients were chosen groupwise to be included in the true model, \(T\), and if \(\mathcal{I}_g \in \mathcal{I}_T\), then the corresponding parameter was given a value of 10. Variables not chosen for the true model, were given random values chosen from a \(N(0, 1)\) distribution. These are intended to be regularized to zero. Simulated \(Y_i\) were drawn from independent Bernoulli distributions where, $$p_\beta(x) = \left[1 + \exp{-\tilde{\mathbf{X}}^T_{i, G} \mathbf{\beta}_G} \right]^{-1}.$$

For the simulations, \(\mathcal{I}_T = \mathcal{I}_{1, 2, 4, 6, 7}\). The figure below shows the selection results for BLBVS with the various values of \(\gamma\) as well as the results for the BootVS without bagging.

\begin{center}
\includegraphics[width=.9\linewidth]{./simresults.png}
\end{center}

The selection results perform equally well under both procedures, even when we set a simple cutoff at being chosen in 50 percent of the replicates. The results show that the proportion estimate becomes more accurate as \(\gamma\), the proportion of the original dataset included in each bagged subset, increases. Logically, the performance of BootVS is superior to BLBVS when \(\gamma\) is small since it resamples from all distinct data points instead of subsets. It has been estimated that each resample in BootVS contains approximately \(0.632n\) different data points in comparison to BootVS's \(n^\gamma\). Thus, it makes sense that the superiority of BootVS decreases as \(\gamma\) increases, and the figure shows that they are effectively equivalent when \(\gamma = 0.8\).

\subsection{{\bfseries\sffamily TODO} Convergence Properties}
\label{sec:org7f3f1e7}
\subsection{{\bfseries\sffamily TODO} Scalability}
\label{sec:org19d03ad}
\subsection{{\bfseries\sffamily TODO} Real Data Analysis}
\label{sec:org9d643a8}

\section{{\bfseries\sffamily TODO} A Scalable Boostrap for Massive Data}
\label{sec:org9801462}
arXiv:1112.5016v2
\end{document}
